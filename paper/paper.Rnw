\documentclass{article}
\title{Word Sense Disambiguation}
\author{
Alec Story
Ansu Abraham
Craig Frey
Dustin Tiedemann
Michael Zhu
Thomas Levine
Whitney Foster
}

\begin{document}
\maketitle


\section{Selecting optimal feature combination}
\newcommand\ward{forward} %or maybe backwards
We used a \ward stepwise approach to indentifying optimal feature combinations.
We first ran the system with all features on, producing a total of $256=2^8$ combinations.
We then used backwards stepwise regression to identify feature combinations
with a small number of features but with high performance

\section{Confidence Cutoffs}

NLTK provides probability measures for each of its guesses when using the
maximum entropy and the naive Bayes learning methods.  We harnessed these to
determine the cutoff for making a guess, or resorting to a ``U.''  Occasional
unknowns also occur because such unknowns are present in the training data, and
we made no effort to remove them, so the learning algorithm views them as any
other label.

% Should we mention which cutoff we used?  Which did we use?

\section{Bootstrapping}

We used the same probability measure from the confidence cutoffs to fuel a
bootstrapping method.  Instead of directly taking the output of the classifier,
we instead looked at its performance on the test data, and wherever it was very
confident, we copied that test instance to the training data, and repeated this
cycle for a fixed number of cycles.  Once we had enriched the training data, we
ran the classifier a final time to perform classification.

Fortunately, the classifiers were very fast, and almost all of the time the
system spends is in reading input and building feature sets, which don't need to
be repeated, so bootstrapping is very cheap to include.

% Should we mention which probability measure we used?  Which did we use?  How
% many iterations?

\end{document}
