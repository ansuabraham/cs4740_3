\documentclass{article}
\usepackage{url}
\title{Word Sense Disambiguation}
\author{
Alec Story
Ansu Abraham
Craig Frey\\
Dustin Tiedemann
Michael Zhu
Thomas Levine
Whitney Foster\\
}

\begin{document}
\maketitle


\section{Selecting optimal feature combination}
\newcommand\ward{forward} %or maybe backwards
We used a \ward stepwise approach to indentifying optimal feature combinations.
We first ran the system with all features on, producing a total of $256=2^8$ combinations.
We then used backwards stepwise regression to identify feature combinations
with a small number of features but with high performance

\section{Confidence Cutoffs}

NLTK provides probability measures for each of its guesses when using the
maximum entropy and the naive Bayes learning methods.  We harnessed these to
determine the cutoff for making a guess, or resorting to a ``U.''  Occasional
unknowns also occur because such unknowns are present in the training data, and
we made no effort to remove them, so the learning algorithm views them as any
other label.

% Should we mention which cutoff we used?  Which did we use?

\section{Bootstrapping}

We used the same probability measure from the confidence cutoffs to fuel a
bootstrapping method.  Instead of directly taking the output of the classifier,
we instead looked at its performance on the test data, and wherever it was very
confident, we copied that test instance to the training data, and repeated this
cycle for a fixed number of cycles.  Once we had enriched the training data, we
ran the classifier a final time to perform classification.

Fortunately, the classifiers were very fast, and almost all of the time the
system spends is in reading input and building feature sets, which don't need to
be repeated, so bootstrapping is very cheap to include.

% Should we mention which probability measure we used?  Which did we use?  How
% many iterations?

\section{Dependency Parsing}

We included as one feature the MaltParser dependency parser, and used the
engmalt.linear.mco linear support vector machine
configuration\footnote{\url{http://maltparser.org/mco/english_parser/engmalt.html}}
for parsing (the other option was a an SVM with a polynomial kernel, which the
Malt website said would just as accurately, more slowly, but with less memory,
so we decided to go with the faster option).

\section{Looking through Different Corpora}

In order to get more data to use with the bootstrapping algorithm, we attempted to search through the Brown corpus and return all of the words that were being disambiguated in the Senseval 3 task.  To do this we needed to find all of the instances of the words that we needed to disambiguate and return a context (list of words) and the position of the word in that list.  We also made sure that we the context that we returned both started at the beginning of the sentence and ended at the end of a sentence and had a minimum of 25 words returned on either side of the word.  This data would then be converted into the form of the data that we received from the Senseval document and ran through the bootstrapping algorithm to have more data to train with.  However when we actually tested this feature alone we found it to be very uninformative.  First, there were very few instances of the words that we were looking for (ex: activate only appeared 3 times), which made searching through the entire corpus to find these words more time consuming than it was helpful.  Therefore, we decided not to use it for the actual test because it was not informative enough by itself.

\end{document}
