\documentclass{article}
\usepackage{url}
\title{Word Sense Disambiguation\\
\small{Leveraging the NLTK Toolkit and External Tools}}
\author{
Alec Story
Ansu Abraham
Craig Frey\\
Dustin Tiedemann
Michael Zhu
Thomas Levine
Whitney Foster\\
}

\begin{document}
\maketitle

\section{Overview}
We  chose to implement our system in Python, and chose to use the machine learning algorithms in the NLTK toolkit which is also Python-based.

We wrote our main control program, classify.py, so that it can choose either [we're just doing nb and decision trees now right?] classifier and any combination of  features. On top of this we wrote bash scripts that automated looping through every combination of classifiers and features, along with porting the results directly into the scorer.
Classify.py includes a help switch -h, that explains the switches that control the different classifiers and features along with any additional parameters that a certain feature might support (for example, collocation has a window size parameter which defaults to 0)
Performance was excellent for most of the features, allowing us to typically run all of the words in the test set through every combination a classifier and feature set in under an hour. (The number of runs can get quite high when you factor in several parameters per feature, not just on/off)

A baseline was run by choosing the most frequent sense for each word.

\subsection{Data details}
The  NLTK has a corpus reader  that imports Senseval-2 files. The Senseval-3  file format is slightly  different which necessitated a few changes. We separated the training  and test files by word into .pos files to match  what the existing  parser looked for. In the interest of time we modified  the training and  test files to contain a single tagged word per chunk  (but still  allowing for multiple senses) again to match the Senseval-2  format.

\subsection{Selecting optimal feature combination}
\newcommand\ward{forward} %or maybe backwards
We used a \ward stepwise approach to indentifying optimal feature combinations.
We first ran the system with all features on, producing a total of $256=2^8$ combinations.
We then used backwards stepwise regression to identify feature combinations
with a small number of features but with high performance

\section{System}

We made heavy use of python's NLTK package\footnote{\url{http://www.nltk.org/}}, using its classifiers as our machine learning engines, and several of its other packages for analysis.

\subsection{Classifiers}

The NLTK was used to implement two machine learning algorithms, naive Bayes and decision trees.  

\subsubsection{Naive Bayes}

The NLTK implements a na√Øve bayes classifier which chooses the best sense given a dictionary of features and assigns confidence probabilities to the chosen sense. Naive bayes does assume an independence of features. The probability of each feature contributes to the overall choice.

\subsubsection{Decision Tree}

The NLTK also implements a decision list classifier in which a sequence of tests is applied to each target-word feature vector. Each test indicates a particular sense of the word: if the test succeeds, the sense of the test is returned otherwise the next test in the sequence is performed.

Naive Bayes supported probability estimation, while the decision tree did not.

\subsection{Confidence Cutoffs}

NLTK provides probability measures for each of its guesses when using the
maximum entropy and the naive Bayes learning methods.  We harnessed these to
determine the cutoff for making a guess, or resorting to a ``U'' if we were unconfident in the tagging.  Occasional unknowns also occur because such unknowns are present in the training data, and we made no effort to remove them, so the learning algorithm views them as any other label.

% Should we mention which cutoff we used?  Which did we use?

\subsection{Bootstrapping}

We used the same probability measure from the confidence cutoffs to fuel a
bootstrapping method.  Instead of directly taking the output of the classifier,
we instead looked at its performance on the test data, and wherever it was very
confident, we copied that test instance to the training data, and repeated this
cycle for a fixed number of cycles.  Once we had enriched the training data, we
ran the classifier a final time to perform classification.

Fortunately, the classifiers were very fast, and almost all of the time the
system spends is in reading input and building feature sets, which don't need to
be repeated, so bootstrapping is very cheap to include.

% Should we mention which probability measure we used?  Which did we use?  How
% many iterations?

\subsection{Base Word}

Because the target words were grouped by part of speech, we included the word as it appears literally in context as a feature.  For example, ``activate'' versus ``activating,'' which both appear under activate.v.

\subsection{Dependency Parsing}

We included as one feature the MaltParser dependency parser, and used the
engmalt.linear.mco linear support vector machine
configuration\footnote{\url{http://maltparser.org/mco/english_parser/engmalt.html}}
for parsing (the other option was an SVM with a polynomial kernel, which the
Malt website said would run just as accurately, more slowly, but with less memory,
so we decided to go with the faster option).

From the parser we rendered several features:
\begin{description}

\item[The Dependency:] whatever the first of the list of dependencies given for the target word is.  It is difficult to represent lists in the key and value feature format, so we did not attempt to.  Instead, we use the rest of the features as mitigating factors.  The empty string if there is no dependency.

\item[Presence of Absence of Dependency]

\item[Number of Dependencies]

\item[The Parent:] the word with the target word as (one of) its dependencies.

\end{description}

Dependency parsing is fairly slow, requiring about a second per instance to run on a computer running an Atom D525 processor (which is fairly underpowered, but modern).  To help alleviate this problem, we wrote a system that saves parses for each context, and only calls the parser if it observes a new parse, so we could reuse parsing information across runs of our program.  Because this file is several hundred megabytes in size, it is not included in our submitted code.

\subsection{Collocation}

Encodes the part-of-speech tag of words to the left and right of the target word. The number of words extracted is determined by a variable window size (context) which the function takes in as an input. Half the window size is taken from left the word and half from the right of the word; thus window size is the total number of words examined. For an odd window size, it rounds up half the window size up for the part left of the word and rounds down for the part right of the word.

We rendered a feature for each position covered, and set its value to be the word at that position.

We also used nltk's part of speech tagger to tag the context, and included those as we did the literal words in the feature set.  This was trained on nltk's \verb+maximum entropy treebank pos tagger+ data, which is included with the nltk package.

\subsection{Cooccurrence}
Looks through the training data and compiles a list of most common words for each item in the data. When the test data is run, the feature looks for the created vector and uses that instead of trying to make a new one. Next the extractor takes a window of words around the head word and generates the 0, 1 tuple of features for inclusion to the dictionary. This word list is used as the keys of the feature vector when using the test data as a list of words that may cooccur with each item in the data. The value associated with each key in the feature vector starts out as 0 (for occurring 0 times in the window near a test word) If a key in the feature vector does occur with a word in the test data, the value associated with the key changes to 1 even if the key occurs more than once.  This is necessary because the machine learning algorithm we used does not attempt to analyze the keys beyond simple equality, so higher numbers than one could have masked the possibly more important distinction between 0 and not-0. In other words, keeping track of the instances of occurrence vs clustering them into 2 groups reduces the sparsity of the data and allows for a higher chance of accurate classifications based on the limited training data that we have. This is an especially important consideration since the feature vectors that select the most common words by looking through all the instances of unique lexical items in the training data are unsupervised.


\section{Methods}
\newcommand\few{few}
We ran the system with several hundred different combinations of features over all the words in the input set.

We identified optimal feature combinations through backward stepwise simple linear regression
and through the creation and visual analysis of charts. The former has the
advantage of discovering relationships and testing them more quantitatively
but has the disadvantage of making assumptions about normality, linearity and equal variance.
The latter has the advantage of elucidating more complex and higher-order relationships.

We ran a more detailed analysis on the top \few systems.
This analysis included consideration of all three grain-levels of score and scores by word.


\section{Results}

\subsubsection{Stepwise regression}
We ran backward stepwise regression with the F-measure from the coarse-grained scores
regressed on predictors representing the feature combination.
The saturated model contained the following predictors and all interactions
\begin{itemize}
\item Classifier (naive Bayes or decision list)
\item Collocation window length (0--8)
\item Cooccurrence handling (on or off)
\item Base word handling (on or off)
\item Bootstrap iterations (0--4)
\item Dependency parsing (on or off)
\end{itemize}
The predictor combination yeilding the lowest value of the Akaike information criterion
included the following parameters.
\begin{itemize}
\item Intercept
\item Classifier
\item Collocation window size
\item Coocurrence
\item Base word handling
\item Classifier$\times$collocation window size interaction
\item Classifier$\times$cooccurrence interaction
\end{itemize}
This does not include bootstrap iterations, indicating that changes
in bootstrap iterations does not substantially change the F-measure.


%Discuss the coefficients in the regression table


%The negative interaction terms may indicate that the interacting features provide
similar functionality and blah blah blah

\subsubsection{Graphical methods}
We began by plotting the predictors that seemed most influential in the
stepwise regression.

%stepwise graph 1
%classifiert, collocation, cooccurrence  
%Interpret this graph

Collocation window of size 2--4 had the best performance, so we looked more closely at those.

%stepwise graph 2

more discussin of graphs

%stepwise graph 33

\subsection{Characterization of the top feature combinations}

%Let's get a comparison between our best performer(s) and the baseline

\section{Discussion}

\subsection{Bootstrapping}

Bootstrapping did not seem to be an effective method for improving accuracy.  This is probably because the input data is not very clean, and additionally our system is probably not very accurately estimating confidence.  When the system is not confident, it bootstraps very few examples, so it does not significantly change the model.

\subsection{Methods We Discarded}

We attempted some preliminary testing on a variety of features, but did not include them because they did not seem to provide insight:

\subsubsection{Sentence Length}
For the sentence containing the word of interest, we extracted sentence length in both characters and words. We ran two preliminary tests to see whether this feature might be useful.

First, we used nearest-neighbor clustering to sort the lengths for a particular word into as many groups as the word had senses. The clusters did not seem to be related to a particular word sense; the chance of a particular instance being in a particular group seemed to be independent of its sense.

Second, we plotted histograms sentence length. The distributions appeared to be unimodal, further suggesting that the lengths could not be nicely grouped into sense clusters, or any clusters for that matter.

\subsubsection{Common Words in Context}
An idea for a feature extractor was to take the most common words in a context of a lexeme and use those words as features. Based off of the cooccurrence code, this would generate a vector from the training data, find the most common words from the test data, and then run the collected data through the classifier.

Unfortunately, this feature extractor did not give very good results. Both the precision and recall for this feature extractor alone on the EnglishLS test data were 0.039. This is to be expected since it is essentially the cooccurrence feature extractor but without grouping data into buckets. The sparsity problem is even more noticeable in this extractor.

\subsubsection{Part of Speech Tagging For Target Words}
We built a part-of-speech tagger and trained it on the Brown corpus. We planned on using it to tag words, but the target words were already grouped by part of speech, (activate.\texttbf{v}) so we did not use it.

%We're using this now, just on a different corpus, right?
%\subsubsection{Bootstrapping on the Brown Corpus} 
%The instances of the target words in the Brown corpus were very few, e.g., activate appeared only 3 times, and bootstrapping didn't seem to be very accurate in general.

\subsubsection{Dependency Parsing}

For all of its sophistication, the dependency parser didn't seem to improve results.  This is probably because most of the relationships caught by the parser would have been caught by the collocation feature extractor as well, since most dependency relations are among nearby words.

\subsubsection{Collocation}

Collocation seemed to be a fairly powerful disambugator, which is to be expected, since function words near to the target word can be very strong predictors of how the word is being used in context.  Effectiveness was strongest with words close to the target, and weaker the farther away we extended the feature.

\subsubsection{Interactions Between Features}

In principle, adding more features should not decrease our accuracy assuming that they're at worst non-predictive, and not somehow anti-predictive, where they fool the machine learning algorithm during training and fail spectacularly on the test corpora.  However, because the size of the datasets is so small, it is probable that the machine learning algorithm has too many features and too few examples to reliably disambiguate between them, which is why we see decreased effectiveness as we turn up the number of features, particularly with the size of the collocation features.  This suggests that, if the training corpus is small, it is appropriate to study which features are the most useful, and only include those, rather than our initial approach of throwing them all together and letting the learning algorithm sort it out.

\end{description}

\end{document}


